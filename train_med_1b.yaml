### --- 1. 模型设置 --- ###
model_name_or_path: /data/ocean/decoding/model/LLM-Research/Llama-3.2-1B-Instruct
stage: sft
do_train: true
finetuning_type: full   # 4×H200 直接全参微调

### --- 2. 数据集设置 --- ###
dataset: med_reason_custom      # 对应 dataset_info.json 里注册的名字
template: llama3                # Llama-3.2 使用 llama3 模板
cutoff_len: 4096                # 医疗推理长文本，尽量保长
val_size: 0.05
overwrite_cache: true


### --- 3. 训练核心参数 --- ###
learning_rate: 2.0e-5
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.1
weight_decay: 0.1
max_grad_norm: 1.0

### --- 4. 显卡并行加速 (4×H200) --- ###
per_device_train_batch_size: 16          # H200 足够，如显存富余可升到 12~16
per_device_eval_batch_size: 8
gradient_accumulation_steps: 4          # 全局 batch = 4卡 * 16 * 4 = 256
bf16: true
gradient_checkpointing: false           # H200 显存充裕，关闭以加速
flash_attn: fa2                         # Hopper 架构已确认安装 flash-attn，可启用 FA2

### --- 5. DeepSpeed 配置 --- ###
deepspeed: examples/deepspeed/ds_z2_config.json  # 默认 ZeRO-2，适合全参

### --- 6. 保存与日志 --- ###
output_dir: /data/ocean/decoding/model/LLM-Research/Llama-3.2-1B-Instruct-medreason-ft
logging_steps: 10
eval_strategy: steps
eval_steps: 500
save_steps: 500
save_total_limit: 3
plot_loss: true